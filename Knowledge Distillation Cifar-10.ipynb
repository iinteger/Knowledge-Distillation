{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import cifar10, cifar100, mnist,fashion_mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Activation,GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "import numpy as np\n",
    "import tensorflow.keras\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import activations\n",
    "\n",
    "np.random.seed(0)\n",
    "def softer_softmax(x, axis=-1):\n",
    "    ndim = K.ndim(x)\n",
    "    if ndim == 1:\n",
    "        raise ValueError('Cannot apply softmax to a tensor that is 1D')\n",
    "    elif ndim == 2:\n",
    "        return K.softmax(x / T)  # normalize with T\n",
    "    elif ndim > 2:\n",
    "        e = K.exp(x - K.max(x, axis=axis, keepdims=True))\n",
    "        s = K.sum(e, axis=axis, keepdims=True)\n",
    "        return e / s\n",
    "    else:\n",
    "        raise ValueError('Cannot apply softmax to a tensor that is 1D. '\n",
    "                         'Received input: %s' % x)\n",
    "\n",
    "        \n",
    "# 사용자 정의 활성화함수는 load가 안되기 때문에 일반 softmax 값을 T가 적용된 값으로 다시 계산\n",
    "def softmax2softer_softmax(x):\n",
    "    y = []\n",
    "\n",
    "    for i in range(len(x)):\n",
    "        distilled_x = []\n",
    "        distilled_xAppend = distilled_x.append\n",
    "        for j in range(len(x[0])):\n",
    "            distilled_xAppend(np.log((x[i][j]) * sum(x[i]))/T)  # 적용된 소프트맥스를 역연산하여 원래의 확률을 구하고, T 로 나누어 정규화\n",
    "\n",
    "        softer_x = []\n",
    "        softer_xAppend = softer_x.append\n",
    "        for j in range(len(x[0])):\n",
    "            softer_xAppend(np.exp(distilled_x[j])/sum(np.exp(distilled_x)))\n",
    "\n",
    "        y.append(softer_x)\n",
    "    y = np.array(y)\n",
    "    return y\n",
    "\n",
    "\n",
    "dataset = cifar10\n",
    "dataset_name = \"cifar10\"\n",
    "(x_train, y_train), (x_test, y_test) = dataset.load_data()\n",
    "img_rows = x_train.shape[1]\n",
    "img_cols = x_train.shape[2]\n",
    "if len(x_train.shape) == 4:\n",
    "    img_channels = x_train.shape[3]\n",
    "else:\n",
    "    img_channels = 1\n",
    "\n",
    "input_shape = (img_rows, img_cols, img_channels)\n",
    "T = 10.0  # T-value\n",
    "batch_size = 512\n",
    "epoch = 100  # 증류가 충분히 될 수 있도록 epoch가 필요\n",
    "dropout_rate = 0.4\n",
    "teacher_dense = 512\n",
    "student_dense = 512\n",
    "num_classes = len(np.unique(y_train))\n",
    "result_acc = []\n",
    "result_loss = []\n",
    "earlystopping = EarlyStopping(monitor=\"val_loss\", patience=10)\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, img_channels)\n",
    "x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, img_channels)\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "\n",
    "y_train = tensorflow.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tensorflow.keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modeling\n",
    "\n",
    "def teacher_model():\n",
    "    print(\"teacher training start\")\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(64, (3, 3), padding=\"same\", input_shape=input_shape))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Conv2D(128, (3, 3), padding=\"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Conv2D(128, (3, 3), padding=\"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Conv2D(256, (3, 3), padding=\"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Conv2D(256, (3, 3), padding=\"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(teacher_dense))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Dense(num_classes, activation=\"softmax\"))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    hist = model.fit(x_train, y_train,\n",
    "                     batch_size=batch_size,\n",
    "                     epochs=epoch,\n",
    "                     verbose=0,\n",
    "                     validation_data=(x_test, y_test),\n",
    "                     callbacks=[earlystopping])\n",
    "\n",
    "    # test_loss, test_acc = model.evaluate(x_test, y_test, verbose=1)\n",
    "\n",
    "    # result_acc.append(test_acc)\n",
    "    # result_loss.append(test_loss)\n",
    "    #\n",
    "    # print('teacher Test loss:', test_loss)\n",
    "    # print('teacher Test accuracy:', test_acc)\n",
    "\n",
    "    model.save(\"models/teacher-\"+dataset_name+\".h5\")\n",
    "    return model, hist\n",
    "\n",
    "# distilled(student) model\n",
    "def student_model1(teacher_model):\n",
    "    print(\"student training start\")\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), padding=\"same\", input_shape=input_shape))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Conv2D(128, (3, 3), padding=\"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Conv2D(128, (3, 3), padding=\"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(student_dense))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Dense(num_classes, activation=softer_softmax))\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    soft_train_pred = teacher_model.predict(x_train)\n",
    "    soft_train_labels = softmax2softer_softmax(soft_train_pred)\n",
    "    soft_test_pred = teacher_model.predict(x_test)\n",
    "    soft_test_labels = softmax2softer_softmax(soft_test_pred)\n",
    "\n",
    "    hist1 = model.fit(x_train, soft_train_labels,\n",
    "                      epochs=epoch,\n",
    "                      batch_size=batch_size,\n",
    "                      verbose=0,\n",
    "                      validation_data=(x_test, soft_test_labels),\n",
    "                      callbacks=[earlystopping])\n",
    "\n",
    "    # 추론은 일반 softmax로 해야하기 때문에 변경 후 다시 load\n",
    "    model.layers[-1].activation = activations.softmax\n",
    "    model.save(\"models/student1-\"+dataset_name+\".h5\")\n",
    "    model = load_model(\"models/student1-\"+dataset_name+\".h5\")\n",
    "    test_loss, test_acc = model.evaluate(x_test, y_test, verbose=1)\n",
    "\n",
    "    result_acc.append(test_acc)\n",
    "    result_loss.append(test_loss)\n",
    "    print('student1 Test loss:', test_loss)\n",
    "    print('student1 Test accuracy:', test_acc)\n",
    "\n",
    "    return hist1\n",
    "\n",
    "# normal model\n",
    "def student_model2():\n",
    "    print(\"normal model training start\")\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), padding=\"same\", input_shape=input_shape))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Conv2D(128, (3, 3), padding=\"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Conv2D(128, (3, 3), padding=\"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(student_dense))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Dense(num_classes, activation=\"softmax\"))\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    hist1 = model.fit(x_train, y_train,\n",
    "                      epochs=epoch,\n",
    "                      batch_size=batch_size,\n",
    "                      verbose=0,\n",
    "                      validation_data=(x_test, y_test),\n",
    "                      callbacks=[earlystopping])\n",
    "\n",
    "    test_loss, test_acc = model.evaluate(x_test, y_test, verbose=1)\n",
    "\n",
    "    result_acc.append(test_acc)\n",
    "    result_loss.append(test_loss)\n",
    "\n",
    "    print('student2 Test loss:', test_loss)\n",
    "    print('student2 Test accuracy:', test_acc)\n",
    "\n",
    "    model.save(\"models/student2-\"+dataset_name+\".h5\")\n",
    "    return hist1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "teacher training start\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000016E421BB3A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000016E421BB3A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000017057815A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000017057815A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "student training start\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x0000017065FB6B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x0000017065FB6B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\data\\lib\\site-packages\\ipykernel_launcher.py:38: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000017068633318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000017068633318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x00000170578154C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x00000170578154C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000017032ECA9D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000017032ECA9D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.5893 - accuracy: 0.8299\n",
      "student1 Test loss: 0.5892620086669922\n",
      "student1 Test accuracy: 0.8299000263214111\n",
      "normal model training start\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000017032D05708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000017032D05708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000017068693D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000017068693D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.6306 - accuracy: 0.8077\n",
      "student2 Test loss: 0.6306177973747253\n",
      "student2 Test accuracy: 0.807699978351593\n"
     ]
    }
   ],
   "source": [
    "# main\n",
    "\n",
    "teacher, teacher_hist = teacher_model()\n",
    "\n",
    "hist1 = student_model1(teacher)  # improved\n",
    "\n",
    "hist2 = student_model2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilled model, normal model\n",
      "acc : [0.8299000263214111, 0.807699978351593]\n",
      "loss : [0.5892620086669922, 0.6306177973747253]\n"
     ]
    }
   ],
   "source": [
    "# result\n",
    "\n",
    "print(\"distilled model, normal model\")\n",
    "print(\"acc :\", result_acc)\n",
    "print(\"loss :\",result_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training visualizing\n",
    "\n",
    "# plt.plot(hist1.history['accuracy'])\n",
    "# plt.plot(hist2.history['accuracy'])\n",
    "# plt.title('model accuracy')\n",
    "# plt.ylabel('accuracy')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['student1', \"student2\"], loc='upper left')\n",
    "# plt.show()\n",
    "#\n",
    "# plt.plot(hist1.history['val_accuracy'])\n",
    "# plt.plot(hist2.history['val_accuracy'])\n",
    "# plt.title('model accuracy')\n",
    "# plt.ylabel('accuracy')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['student1', \"student2\"], loc='upper left')\n",
    "# plt.show()\n",
    "#\n",
    "# plt.plot(hist1.history['loss'])\n",
    "# plt.plot(hist2.history['loss'])\n",
    "# plt.title('model loss')\n",
    "# plt.ylabel('loss')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['student1', \"student2\"], loc='upper left')\n",
    "# plt.show()\n",
    "#\n",
    "# plt.plot(hist1.history['val_loss'])\n",
    "# plt.plot(hist2.history['val_loss'])\n",
    "# plt.title('model loss')\n",
    "# plt.ylabel('loss')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['student1', \"student2\"], loc='upper left')\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
